# -*- coding: utf-8 -*-
"""VGG_MNISTprepped.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hMmMAwuT_-RfoGspv1pPWhn1BhphH-BF
"""

import torch.nn.functional as F
import torch
import torchvision
from matplotlib import pyplot as plt
# import intel_extension_for_pytorch as ipex
from codecarbon import EmissionsTracker

# pip install pybind11

root_dir = './'
torchvision.datasets.MNIST(root=root_dir,download=True)
tracker = EmissionsTracker()

# Load MNIST dataset
train_set = torchvision.datasets.MNIST(root=root_dir, download=True, train=True)
test_set = torchvision.datasets.MNIST(root=root_dir, download=True, train=False)

# Dataset len
num_train = len(train_set)
num_test = len(test_set)
print(f"Num. training samples: {num_train}")
print(f"Num. test samples:     {num_test}")

# Commented out IPython magic to ensure Python compatibility.
from torchvision import utils
import matplotlib.pyplot as plt
import numpy as np
# %matplotlib inline

# extract data and targets
x_train, y_train=train_set.data,train_set.targets
print(type(x_train))
print(x_train.shape)
print(y_train.shape)

# add a dimension to tensor to become B*C*H*W
if len(x_train.shape)==3:
  x_train=x_train.unsqueeze(1)
print(x_train.shape)

def show(img):
  # convert tensor to numpy array
  npimg = img.numpy()
  # Convert to H*W*C shape
  npimg_tr=np.transpose(npimg, (1,2,0))
  plt.imshow(npimg_tr,interpolation='nearest')

# # make a grid of 40 images, 8 images per row
# x_grid=utils.make_grid(x_train[:40], nrow=8, padding=2)
# print(x_grid.shape)
# # call helper function
# show(x_grid)

# List of indexes on the training set
train_idx = list(range(num_train))

# List of indexes of the test set
test_idx = list(range(num_test))

# Shuffle the training set
import random

random.shuffle(train_idx)
for i in range(10):
  print(train_idx[i])

# Fraction of the original train set that we want to use as validation set
val_frac = 0.1
# Number of samples of the validation set
num_val = int(num_train * val_frac)
num_train = num_train - num_val

# Split training set
val_idx = train_idx[num_train:]
train_idx = train_idx[:num_train]

print(f"{num_train} samples used as train set")
print(f"{num_val}  samples used as val set")

len(train_idx)

from torchvision import transforms

# Compose transformations
data_transform = transforms.Compose([
  transforms.Resize(32),
  transforms.RandomHorizontalFlip(),
  transforms.ToTensor(),
])

test_transform = transforms.Compose([
  transforms.Resize(32),
  transforms.ToTensor(),
])
# Load MNIST dataset with transforms
train_set = torchvision.datasets.MNIST(root=root_dir, train=True, download=True, transform=data_transform)
test_set = torchvision.datasets.MNIST(root=root_dir, train=False, download=True, transform=test_transform)

image, label = train_set[1]
print(image.shape)
plt.imshow(image.squeeze(), cmap='gray')
print('Label:', label)

# Split train_dataset into training and validation
from torch.utils.data import Subset

val_set = Subset(train_set, val_idx)
train_set = Subset(train_set, train_idx)
print(train_set)

# Define loaders
from torch.utils.data import DataLoader
train_loader = DataLoader(train_set, batch_size=64, num_workers=2, shuffle=True, drop_last=True)
val_loader   = DataLoader(val_set,   batch_size=64, num_workers=2, shuffle=False, drop_last=False)
test_loader  = DataLoader(test_set,  batch_size=64, num_workers=2, shuffle=False, drop_last=False)



import torch.nn as nn

class VGGBlock(nn.Module):
    def __init__(self, in_channels, out_channels,batch_norm=False):

        super().__init__()

        conv2_params = {'kernel_size': (2, 2),
                        'stride'     : (1, 1),
                        'padding'   : 1
                        }

        noop = lambda x : x

        self._batch_norm = batch_norm

        self.conv1 = nn.Conv2d(in_channels=in_channels,out_channels=out_channels , **conv2_params)

        self.bn1 = nn.BatchNorm2d(out_channels) if batch_norm else noop
        #self.bn1 = nn.GroupNorm(32, out_channels) if batch_norm else noop

        self.conv2 = nn.Conv2d(in_channels=out_channels,out_channels=out_channels, **conv2_params)
        self.bn2 = nn.BatchNorm2d(out_channels) if batch_norm else noop
        #self.bn2 = nn.GroupNorm(32, out_channels) if batch_norm else noop

        self.max_pooling = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))

    @property
    def batch_norm(self):
        return self._batch_norm

    def forward(self,x):

        x = self.conv1(x)

        x = self.bn1(x)
        x = F.relu(x)

        x = self.conv2(x)
        x = self.bn2(x)
        x = F.relu(x)

        x = self.max_pooling(x)

        return x

class VGG16(nn.Module):

  def __init__(self, input_size, num_classes=10,batch_norm=False):
    super(VGG16, self).__init__()

    self.in_channels,self.in_width,self.in_height = input_size

    self.block_1 = VGGBlock(self.in_channels,64,batch_norm=batch_norm)
    self.block_2 = VGGBlock(64, 128,batch_norm=batch_norm)
    self.block_3 = VGGBlock(128, 256,batch_norm=batch_norm)
    self.block_4 = VGGBlock(256,512,batch_norm=batch_norm)


  @property
  def input_size(self):
      return self.in_channels,self.in_width,self.in_height

  def forward(self, x):

    x = self.block_1(x)
    x = self.block_2(x)
    x = self.block_3(x)
    x = self.block_4(x)

    return x

# Create the model
model = VGG16((1,32,32),batch_norm=True)
print(model)

# Get an element from the dataset
test_x, _ = train_set[100] # each element of the dataset is a couple (image, label)
test_x = test_x.unsqueeze(dim=0)

# Get the size of a sample
test_x.size()

test_x_np = test_x.numpy()
print(test_x_np)
test_x_flattened = test_x_np.ravel()

# Writing the flattened array to a text file
with open('flat_test_x.txt', 'w') as file:
    for value in test_x_flattened:
        file.write(str(value) + '\n')

print(test_x_flattened[232])

def apply_conv2d(batch):
    # Check if CUDA is available and then set the device to GPU
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f'Using device: {device}')

    # Define a Conv2D layer

    # Parameters: in_channels (1), out_channels (64), kernel_size (default is 2)
    conv_layer = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=2, padding=1, stride=1)

    # Manually initialize weights to a fixed value - 0.1
    nn.init.constant_(conv_layer.weight, 0.1)

    # initialize biases to a fixed value
    nn.init.constant_(conv_layer.bias, 0)

    # current_weights_numpy = conv_layer.weight.data.cpu().numpy()

    # print(current_weights_numpy)

    # Apply the conv layer to the batch
    # Assuming the batch is of the shape (batch_size, channels, height, width)
    output = conv_layer(batch)

    # Move the conv layer to the GPU
    conv_layer = conv_layer.to(device)

    # Move the batch to the GPU
    batch = batch.to(device)

    start_time = time.time()
    tracker.start()
    # Apply the conv layer to the batch
    output = conv_layer(batch)
    end_time = time.time()
    total_time = end_time - start_time
    emission:float =tracker.stop()
    print(f"Total running time: {total_time} seconds")
    print(f"Emission: {emission} kg")
    return output

import time

start_time = time.time()
output = apply_conv2d(test_x)
end_time = time.time()
total_time = end_time - start_time
# print(f"Total running time: {total_time} seconds")

print(output.shape)
print(test_x[0])

###
run_kn2row = """#!/bin/bash
source /opt/intel/oneapi/setvars.sh > /dev/null 2>&1
/bin/echo "##" $(whoami) is compiling kn2row
icpx -fsycl -fsycl-device-code-split=per_kernel -DMKL_ILP64 -I$MKLROOT/include -L$MKLROOT/lib/intel64 -lmkl_sycl -lmkl_intel_ilp64 -lmkl_sequential -lmkl_core -lsycl -lOpenCL -lpthread -lm -ldl kn2row_conv_nonsycl.cpp -o kn2row_conv_nonsycl
if [ $? -eq 0 ]; then ./kn2row_conv_nonsycl; fi
"""

with open("run_kn2row.sh", "w") as f:
    f.write(run_kn2row)

# !chmod 755 q; chmod 755 run_kn2row.sh; if [ -x "$(command -v qsub)" ]; then ./q run_kn2row.sh; else ./run_knw2row_alt.sh; fi













"""class VGG16(nn.Module):

  def __init__(self, input_size, num_classes=10,batch_norm=False):
    super(VGG16, self).__init__()

    self.in_channels,self.in_width,self.in_height = input_size

    self.block_1 = VGGBlock(self.in_channels,64,batch_norm=batch_norm)
    self.block_2 = VGGBlock(64, 128,batch_norm=batch_norm)
    self.block_3 = VGGBlock(128, 256,batch_norm=batch_norm)
    self.block_4 = VGGBlock(256,512,batch_norm=batch_norm)

    self.classifier = nn.Sequential(
            nn.Linear(2048, 4096),
            nn.ReLU(True),
            nn.Dropout(p=0.65),
            nn.Linear(4096, 4096),
            nn.ReLU(True),
            nn.Dropout(p=0.65),
            nn.Linear(4096, num_classes)
        )

  @property
  def input_size(self):
      return self.in_channels,self.in_width,self.in_height

  def forward(self, x):

    x = self.block_1(x)
    x = self.block_2(x)
    x = self.block_3(x)
    x = self.block_4(x)
    x = x.view(x.size(0), -1)
    x = self.classifier(x)

    return x
"""



# Create the model
model = VGG16((1,32,32), batch_norm=True)
output = model(test_x)
output.shape

torch.cuda.is_available()

dev = torch.device('xpu')
print(dev)

# Define an optimizier
import torch.optim as optim
optimizer = optim.SGD(model.parameters(), lr = 0.01)
# Define a loss
criterion = nn.CrossEntropyLoss()

def train(net, loaders, optimizer, criterion, epochs=20, dev=dev, save_param = False, model_name="valerio"):
    try:
        net = net.to(dev)
        #print(net)
        # Initialize history
        history_loss = {"train": [], "val": [], "test": []}
        history_accuracy = {"train": [], "val": [], "test": []}
        # Store the best val accuracy
        best_val_accuracy = 0

        # Process each epoch
        for epoch in range(epochs):
            # Initialize epoch variables
            sum_loss = {"train": 0, "val": 0, "test": 0}
            sum_accuracy = {"train": 0, "val": 0, "test": 0}
            # Process each split
            for split in ["train", "val", "test"]:
                if split == "train":
                  net.train()
                else:
                  net.eval()
                # Process each batch
                for (input, labels) in loaders[split]:
                    # Move to CUDA
                    input = input.to(dev)
                    labels = labels.to(dev)
                    # Reset gradients
                    optimizer.zero_grad()
                    # Compute output
                    pred = net(input)
                    #pred = pred.squeeze(dim=1) # Output shape is [Batch size, 1], but we want [Batch size]
                    #labels = labels.unsqueeze(1)
                    labels = labels.long()
                    loss = criterion(pred, labels)
                    # Update loss
                    sum_loss[split] += loss.item()
                    # Check parameter update
                    if split == "train":
                        # Compute gradients
                        loss.backward()
                        # Optimize
                        optimizer.step()
                    # Compute accuracy
                    #pred_labels = pred.argmax(1) + 1
                    #pred_labels = (pred >= 0.5).long() # Binarize predictions to 0 and 1
                    _,pred_label = torch.max(pred, dim = 1)
                    pred_labels = (pred_label == labels).float()

                    batch_accuracy = pred_labels.sum().item()/input.size(0)
                    # Update accuracy
                    sum_accuracy[split] += batch_accuracy
            # Compute epoch loss/accuracy
            epoch_loss = {split: sum_loss[split]/len(loaders[split]) for split in ["train", "val", "test"]}
            epoch_accuracy = {split: sum_accuracy[split]/len(loaders[split]) for split in ["train", "val", "test"]}

            # Store params at the best validation accuracy
            if save_param and epoch_accuracy["val"] > best_val_accuracy:
              #torch.save(net.state_dict(), f"{net.__class__.__name__}_best_val.pth")
              torch.save(net.state_dict(), f"{model_name}_best_val.pth")
              best_val_accuracy = epoch_accuracy["val"]

            # Update history
            for split in ["train", "val", "test"]:
                history_loss[split].append(epoch_loss[split])
                history_accuracy[split].append(epoch_accuracy[split])
            # Print info
            print(f"Epoch {epoch+1}:",
                  f"TrL={epoch_loss['train']:.4f},",
                  f"TrA={epoch_accuracy['train']:.4f},",
                  f"VL={epoch_loss['val']:.4f},",
                  f"VA={epoch_accuracy['val']:.4f},",
                  f"TeL={epoch_loss['test']:.4f},",
                  f"TeA={epoch_accuracy['test']:.4f},")
    except KeyboardInterrupt:
        print("Interrupted")
#     finally:
#         # Plot loss
#         plt.title("Loss")
#         for split in ["train", "val", "test"]:
#             plt.plot(history_loss[split], label=split)
#         plt.legend()
#         plt.show()
#         # Plot accuracy
#         plt.title("Accuracy")
#         for split in ["train", "val", "test"]:
#             plt.plot(history_accuracy[split], label=split)
#         plt.legend()
#         plt.show()

# Define dictionary of loaders
loaders = {"train": train_loader,
           "val": val_loader,
           "test": test_loader}

# Train model
# train(model, loaders, optimizer, criterion, epochs=10, dev=dev)

